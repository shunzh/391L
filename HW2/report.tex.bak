\documentclass[10pt]{article}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{amsmath}

\title{Homework 2: Independent Component Analysis}
\author{Name: Shun Zhang\\
Email address: \texttt{jensen.zhang@utexas.edu}\\
EID: \texttt{sz4554}}
\date{}

\begin{document}
\maketitle

\section{Independent Component Analysis}

In this report, I applied Independent Component Analysis on Blind Source
Separation problem. Let $U$ be the original sources, which is a $n \times t$
matrix. $n$ is the number of sources and $t$ is the number of data. Let $A$
be mixing operation, and $X = A U$. The goal is to find a matrix $W$ such
that $U = W X$, which recover the sources from the observations. 

I used the batch update method described on the webpage (with the mistake
fixed),
\begin{equation}
	\label{eq:dana}
	\Delta W = \eta (tI + (1 - 2Z) Y^T)W
\end{equation}
This is slightly different from the update equation proved in Ng's note.
\begin{equation}
	\Delta W = \eta ((1 - 2Z) x^T + (W^T)^{-1})
	\label{eq:ng}
\end{equation}
where $x$ stands for an observation at one time. It uses one sample to
update I first convert it to the batch version. Data are added as columns,
which make a matrix $X$. $W^T$ is simply added $t$ times. So,
\begin{equation}
	\Delta W = \eta ((1 - 2Z) X^T + t(W^T)^{-1})
	\label{eq:ng}
\end{equation}
Equation~\ref{eq:dana} is more efficient, as there is no inverse of matrix.
I tried to derive Equation~\ref{eq:dana} from Equation~\ref{eq:ng},
\begin{align}
	\Delta W &= \eta ((1 - 2Z) X^T + t(W^T)^{-1}) \\
	&= \eta ((1 - 2Z) X^T + t(W^T)^{-1})W^T (W^T)^{-1} \\
	&= \eta ((1 - 2Z) X^T W^T + tI)(W^T)^{-1} \\
	&= \eta ((1 - 2Z) (W X)^T + tI)(W^T)^{-1} \\
	&= \eta ((1 - 2Z) Y^T + tI)W (W^{-1})(W^T)^{-1} \\
	&= \eta ((1 - 2Z) Y^T + tI)W (W^TW)^{-1}
	\label{eq:equiv}
\end{align}
So the different is the term, $W^TW$. If $W$ is othognal, then it's $W^TW =
I$. We know this is not a valid assumption. However, I still used
Equation~\ref{eq:dana} in the following experiments.

\section{Experiment}

\subsection{Experiments on Small Set}

\begin{figure}[h]
\centering
\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\textwidth]{rep5.png}
\end{subfigure}
\begin{subfigure}{0.49\textwidth}
	\includegraphics[width=\textwidth]{rep6.png}
\end{subfigure}
\caption{The bottom 3 lines are original signals from
\texttt{icaTest.mat}. The top 3 lines are reconstructed signals with $\eta
= 0.01$ and 1000 iterations. Two independent experiments are shown.}
\label{fig:rep}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{| l l l l |}
\hline
& Recon. 1& Recon. 2& Recon. 3\\
Source 1&-0.4900& \textbf{0.9905}& -0.4248\\
Source 2&\textbf{0.9918}& -0.3957& -0.5454\\
Source 3&-0.4829& -0.5073& \textbf{0.9924}\\
\hline
\end{tabular}
\caption{Linear correlation between source and reconstructed signals, for
the left figure in Figure~\ref{fig:rep}. The maximum correlation is
highlighted.}
\label{tbl:corr1}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{| l l l l |}
\hline
& Recon. 1& Recon. 2& Recon. 3\\
Source 1&-0.4248& \textbf{0.9905}&-0.4900\\
Source 2&-0.5454&-0.3957& \textbf{0.9918}\\
Source 3&\textbf{0.9924}&-0.5073&-0.4829\\
\hline
\end{tabular}
\caption{Linear correlation between source and reconstructed signals, for
the right figure in Figure~\ref{fig:rep}. The maximum correlation is
highlighted.}
\label{tbl:corr2}
\end{table}

First I did experiment on \texttt{icaTest.mat}. $\eta = 0.01$ and 1000
iterations. The results are in shown in Figure~\ref{fig:rep}. 

The results are scaled into $[0, 1]$ interval. This experiment is repeated
twice. Actually, the result can be permutation of the original sources.
Scaling is also possible (as the results are scaled into $[0, 1]$, the
signal could be flipped in this case, though I didn't observe this in the
experiment). The correlation between source and reconstructed signals are
shown in Table~\ref{tbl:corr1} and \ref{tbl:corr2}. By definition, they are
highly correlated if the correlations is close to $1$ or $-1$. We can tell
that, for the left figure, the mapping is 0-4, 1-5, 2-3. While in the right
one, the mapping is 0-5, 1-3, 2-4. This result can also be verified visually.
We can also observe that the algorithm overall generate highly close signal,
with has more than $0.99$ correlations with its source signal.

\begin{figure}[h]
\centering
\includegraphics[width=.6\textwidth]{detW.png}
\caption{$||\Delta W||$ over number of iterations. Average of 100 runs. The
length of vertical bars is $\sigma$ assuming Gaussian distribution of data
points at each iteration.}
\label{fig:detW}
\end{figure}

I also examine how progress is made in each iteration in the learning
process. As the algorithm uses gradient descent, the update on $W$ each
step, which is $\Delta W$, is useful. For the convenience of visualization,
I need the magnitude of this matrix. I use the largest singular value here,
which can be got using \texttt{norm} function when applied to a matrix in
Matlab.

The result is shown in Figure~\ref{fig:detW}. Not surprisingly, steps
become smaller after more iterations. The algorithm converges after $2
\times 10^5$ iterations.

\subsection{Experiments on Sound}

\begin{figure}[h]
\centering
\includegraphics[width=.8\textwidth]{soundfull.png}
\caption{The bottom 3 lines are original signals from
\texttt{sounds.mat}. The top 3 lines are reconstructed signals with $\eta =
10^{-6}$ and 200 iterations.}
\label{fig:sound}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{| l l l l l l |}
\hline
& Recon. 1& Recon. 2& Recon. 3& Recon. 4& Recon. 5\\
Source 1 &-0.0032& 0.0030&-0.0076&-0.0035& \textbf{1.0000}\\
Source 2 &-0.0014&-0.0003& \textbf{0.9999}&-0.0126& 0.0016\\
Source 3 &-0.0026&-0.0095& 0.0126& \textbf{0.9999}& 0.0007\\
Source 4 &0.0020& \textbf{0.9999}& 0.0083&-0.0102& 0.0041\\
Source 5 &\textbf{0.9999}& 0.0051& 0.0020& 0.0125&-0.0014\\
\hline
\end{tabular}
\caption{Linear correlation between source and reconstructed signals.}
\label{tbl:corrs}
\end{table}

The data are from \texttt{sounds.mat}. Matrix $A$ is generated randomly, in
the range of $[0, 1]$ for each element. I set $\eta = 10 ^ {-6}$ as $t$ is
large in one step. It runs 200 iterations. Result on sound are shown in
Figure~\ref{fig:sound}. The correlation result is shown in
Table~\ref{tbl:corrs}. The scaling is same as the previous experiment.

\section{Discussion and Conclusion}

I used a batch version of ICA for processing sound information. The number
of samples is 44000. It's feasible process these samples as a whole.
Actually, when the number of samples is even larger, or when the learning
is online, it would be better to use sample-wise version.

Equation~\ref{eq:dana} is not an accurate one. Experiments have shown that
it's useful. It mainly avoids inverse of an matrix. 

I also set a static number of iterations for both experiments. I can also
set a threshold on $||\Delta W||$ to let it stop.

\end{document}
