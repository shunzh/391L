\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}

\title{Homework 4: Approximate Inference in Bayesian Networks}
\author{Shun Zhang}
\date{}

\begin{document}
\maketitle

\section{Sampling Methods}

In this assignment, I implemented Rejection Sampling and Gibbs
Sampling to generate samples for a Bayesian network. 

For Rejection Sampling, the implementation is quite simple. Samples
are generated ignoring the constraints of the evidence. Then, samples
inconsistent with the evidence are dropped. In the remaining samples,
proportion of samples consistent with the query is returned.

For Gibbs Sampling, in each iteration, for node $x_i$, it is sampled
from the following probability (revised from the textbook).

\begin{equation}
p(x_i|x_{\{j \not = i\}}) \propto \prod_k P(x_k|pa_k)
\end{equation}

\section{Experiments}

\begin{figure}
\centering
\begin{subfigure}[b]{0.49\textwidth}
	\includegraphics[width=\textwidth]{ian1.png}
	\caption{$P(X_0=1 | E_1) $}
	\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
	\includegraphics[width=\textwidth]{ian2.png}
	\caption{$P(X_1=1 | E_1) $}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
	\includegraphics[width=\textwidth]{ian3.png}
	\caption{$P(X_2=1 | E_1) $}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
	\includegraphics[width=\textwidth]{ian4.png}
	\caption{$P(X_3=1 | E_1) $}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
	\includegraphics[width=\textwidth]{ian5.png}
	\caption{$P(X_4=1 | E_1) $}
\end{subfigure}
\begin{subfigure}[b]{0.49\textwidth}
	\includegraphics[width=\textwidth]{ian6.png}
	\caption{$P(X_5=1 | E_1) $}
\end{subfigure}
\caption{Sampling process for each test case. $E_1$ denotes $(X_6 = 1,
X_7 = 0, X_8 = 0)$.}
\label{fig:lp}
\end{figure}

In this experiment, I used the data provided by Ian. For Gibbs
sampling, the parameter $T = 200$. The algorithm starts collect data
after $T$ iteraions. The results generated are shown below.

In Figure~\ref{fig:lp}, I show the learning process in each sampling
algorithms, for each query data for the first envidence given.

The following is the numerical results.

\begin{verbatim}
   0.848837   0.538462   0.142857   0.638298   0.607143   0.750000
   0.795082   0.766129   0.132231   0.990566   0.393939   0.170213
   0.862069   0.642857   0.134021   0.515789   0.450000   0.240385
   0.831579   0.644689   0.130268   0.944649   0.350195   0.847059
   0.930000   0.715909   0.098901   0.495050   0.403670   0.282828
   0.828125   0.648649   0.094595   1.000000   0.742857   0.810811
   0.849462   0.571429   0.122449   0.394737   0.554348   0.611111
   0.825688   0.791304   0.148515   0.943925   0.333333   0.111111
   0.773585   0.696203   0.109091   0.964912   0.731707   0.745098
   0.855263   0.533333   0.117155   0.519824   0.427928   0.836538
\end{verbatim}

\section{Discussion}

The rejection sampling is less efficient in general, because it
generates samples inconsistent with the evidence. This depends on the
probability of the evidence - if the evidence is very unlikely, then
the algorithm needs to run many times to gather enough samples
consistent with the evidence.

In this sense, Gibbs sampling beats rejection sampling because every
sample counts. However, Gibbs sampling relies on the initial sample in
the first few iterations. It needs some iterations to reach a
stationary distribution.

\end{document}
