\documentclass[10pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{qtree}

\title{CS 391L Machine Learning Assignment 3}
\author{Name: Shun Zhang\\
Email address: \texttt{jensen.zhang@utexas.edu}\\
EID: \texttt{sz4554}}
\date{}

\begin{document}
\maketitle

\section*{Problem 1}

\begin{enumerate}[(a)]

\item \begin{align}
Cov(y) &= E((y - E(y)(y - E(y)^T)\\
&= E((Ax + b - E(Ax + b))(Ax + b - E(Ax + b))^T)\\
&= E((Ax - E(Ax))(Ax - E(Ax))^T) &\text{Linearity of $E$}\\
&= AE((x - E(x))(x - E(x))^T)A^T\\
&= A\Sigma A^T &\text{Def. of Cov.}
\end{align}

\item Base case: by the defition of eigenvalue and eigenvector, $Ax =
\lambda x$.

Inductive hypothesis: assume $A^k x = \lambda^k x$ for some $k \in N$.
Want to show $A^{k+1} x = \lambda^{k+1} x$.
\begin{align}
A^k x &= \lambda^k x &\text{I.H.}\\
A^{k+1} x &= A \lambda^k x\\
A^{k+1} x &= \lambda^k Ax\\
A^{k+1} x &= \lambda^{k+1} x &Ax = \lambda x
\end{align}

\end{enumerate}

\section*{Problem 2}

\begin{enumerate}[(a)]

\item 
\begin{align}
r &= 1 - \dfrac{H(Y|X)}{H(X)}\\
&= \dfrac{H(X) - H(Y|X)}{H(X)}\\
&= \dfrac{I(Y|X)}{H(X)} &\text{Def. of Mutual Information}
\end{align}

\item $H(Y|X) \geq 0$, $H(X) > 0$. So $\dfrac{H(Y|X)}{H(X)} \geq 0$,
$1 - \dfrac{H(Y|X)}{H(X)} \leq 1$.

$H(Y|X) < H(X)$, so $\dfrac{H(Y|X)}{H(X)} <= 1$, $1 -
\dfrac{H(Y|X)}{H(X)} \geq 0$.

Therefore, $0 \leq r \leq 1$.

\item $\dfrac{H(Y|X)}{H(X)} = 1$, 

\end{enumerate}

\section*{Problem 3}
\begin{enumerate}[(a)]

\item  $\tanh x = \dfrac{\sinh x}{\cosh x} = \dfrac{e^x - e^{-x}}{e^x
+ e^{-x}} = \dfrac{1 - e^{-2x}}{1 + e^{-2x}}$. Compared to $\dfrac{1}{1
+ e^{-x}}$, $\tanh$ has a steeper slope.

To be an appropriate sigmoid function, $\tanh$ needs to be scaled to
range of [0, 1].

\item We know $W^k$ is $2 \times 2$.
\begin{align}
\dfrac{\partial H}{\partial w_{ij}^k} &= \dfrac{\partial}{\partial
w_{ij}^k} \sum\limits_{k=0}^{K-1} (\lambda^{k+1})^T g(W^kx^k)\\
&= \dfrac{\partial}{\partial w_{ij}^k} (\lambda_1^{k+1} g(w_{11}x_1^k
+ w_{12}x_2^k) + \lambda_2^{k+1} g(w_{21}x_1^k + w_{22}x_2^k))\\
&= \lambda_1^{k+1} \dfrac{\partial}{\partial w_{ij}^k} g(w_{11}x_1^k
+ w_{12}x_2^k) + \lambda_2^{k+1} \dfrac{\partial}{\partial w_{ij}^k}
g(w_{21}x_1^k + w_{22}x_2^k)
\end{align}

Therefore, for $w_{11}, w_{12}, w_{21}, w_{22}$, there is
$\dfrac{\partial H}{\partial w_{ij}^k} = \lambda^{k+1} x_j^k g'(w_i^k x^k)$.

\end{enumerate}

\section*{Problem 4}

\begin{enumerate}[(a)]
\item The code for computing these results is attached separately.

IG(Color) = 0.1043\\
IG(Size) = 0.4086\\
IG(Noise) = 0.0207\\

For small size,

IG(Color) = 0.3219\\
IG(Noise) = 0.0207\\

For medium size,

IG(Color) = 0.1226\\
IG(Noise) = 0.1226\\

For large size, IG is clearly 0.

The decision tree is

\Tree [.Size
  [.\textit{Small}\\Color 
  	[.\textit{Red}\\\textbf{Yes} ]
	[.\textit{Blue}\\Noise
	  [.\textit{Loud}\\\textbf{Yes} ]
	  [.\textit{Quiet}\\\textbf{Yes} ]
	]
  ]
  [.\textit{Medium}\\Color 
  	[.\textit{Red}\\Noise
	  [.\textit{Loud} ]
	  [.\textit{Quiet} ]
	]
	[.\textit{Blue}\\Noise
	  [.\textit{Loud} ]
	  [.\textit{Quiet} ]
	]
  ]
  [.\textit{Medium}\\\textbf{No} ]
]

\item If the event of missing a datum is uniformly random over all the
attributes, then it doesn't harm if we simply delete that line. 

\end{enumerate}

\section*{Problem 5}

\begin{enumerate}[(a)]

\item $A = \pi rs + \pi r^2$. Therefore,

$H = \frac{1}{3} \pi r^2 h + \lambda(\pi rs + \pi r^2 - A)$

$\frac{\partial H}{\partial r} = \frac{2}{3} \pi r h + \lambda(\pi(s
+ r \frac{1}{2s} 2r) + 2\pi r) = 0$

$\frac{\partial H}{\partial h} = \frac{1}{3} \pi r^2 + \lambda(\pi r
\frac{1}{2s} 2h) = 0$

\end{enumerate}

\end{document}
