\documentclass[10pt]{article}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{qtree}

\title{CS 391L Machine Learning Assignment 3}
\author{Name: Shun Zhang\\
Email address: \texttt{jensen.zhang@utexas.edu}\\
EID: \texttt{sz4554}}
\date{}

\begin{document}
\maketitle

\section*{Problem 1}

\begin{enumerate}[(a)]

\item \begin{align}
Cov(y) &= E((y - E(y)(y - E(y)^T)\\
&= E((Ax + b - E(Ax + b))(Ax + b - E(Ax + b))^T)\\
&= E((Ax - E(Ax))(Ax - E(Ax))^T) &\text{Linearity of $E$}\\
&= AE((x - E(x))(x - E(x))^T)A^T\\
&= A\Sigma A^T &\text{Def.\ of Cov.\ }
\end{align}

\item Base case: by the definition of eigenvalue and eigenvector, $Ax =
\lambda x$.

Inductive hypothesis: assume $A^k x = \lambda^k x$ for some $k \in N$.
Want to show $A^{k+1} x = \lambda^{k+1} x$.
\begin{align}
A^k x &= \lambda^k x &\text{I.H.}\\
A^{k+1} x &= A \lambda^k x\\
A^{k+1} x &= \lambda^k Ax\\
A^{k+1} x &= \lambda^{k+1} x &Ax = \lambda x
\end{align}
% negative

\end{enumerate}

\section*{Problem 2}

\begin{enumerate}[(a)]

\item 
\begin{align}
r &= 1 - \dfrac{H(Y|X)}{H(X)}\\
&= \dfrac{H(X) - H(Y|X)}{H(X)}\\
&= \dfrac{I(Y|X)}{H(X)} &\text{Def.\ of Mutual Information}
\end{align}

\item $H(Y|X) \geq 0$, $H(X) > 0$. So $\dfrac{H(Y|X)}{H(X)} \geq 0$,
$1 - \dfrac{H(Y|X)}{H(X)} \leq 1$.

$H(Y|X) < H(X)$, so $\dfrac{H(Y|X)}{H(X)} <= 1$, $1 -
\dfrac{H(Y|X)}{H(X)} \geq 0$.

Therefore, $0 \leq r \leq 1$.

\item $r = 0$ when two variables are independent. $r = 1$ when two
variables are perfectly correlated.

\end{enumerate}

\section*{Problem 3}
\begin{enumerate}[(a)]

\item  $\tanh x = \dfrac{\sinh x}{\cosh x} = \dfrac{e^x - e^{-x}}{e^x
+ e^{-x}} = \dfrac{1 - e^{-2x}}{1 + e^{-2x}}$. Compared to $\dfrac{1}{1
+ e^{-x}}$, $\tanh$ has a steeper slope.

To be an appropriate sigmoid function, $\tanh$ needs to be scaled to
range of [0, 1].

\item We know $W^k$ is $2 \times 2$.
\begin{align}
\dfrac{\partial H}{\partial w_{ij}^k} &= \dfrac{\partial}{\partial
w_{ij}^k} (\lambda^{k+1})^T g(W^kx^k)\\
&= \dfrac{\partial}{\partial w_{ij}^k} (\lambda_1^{k+1} g(w_{11}x_1^k
+ w_{12}x_2^k) + \lambda_2^{k+1} g(w_{21}x_1^k + w_{22}x_2^k))\\
&= \lambda_1^{k+1} \dfrac{\partial}{\partial w_{ij}^k} g(w_{11}x_1^k
+ w_{12}x_2^k) + \lambda_2^{k+1} \dfrac{\partial}{\partial w_{ij}^k}
g(w_{21}x_1^k + w_{22}x_2^k)
\end{align}

Therefore, for $w_{11}, w_{12}, w_{21}, w_{22}$, there is
$\dfrac{\partial H}{\partial w_{ij}^k} = \lambda^{k+1} x_j^k g'(w_i^k x^k)$.

\end{enumerate}

\section*{Problem 4}

\begin{enumerate}[(a)]
\item The code for computing these results is attached separately.

IG(Color) = 0.1043\\
IG(Size) = 0.4086\\
IG(Noise) = 0.0207\\

For small size,

IG(Color) = 0.3219\\
IG(Noise) = 0.0729\\

For medium size,

IG(Color) = 0.1226\\
IG(Noise) = 0.1226\\

For large size, IG is clearly 0.

The decision tree is

\Tree [.Size
  [.\textit{Small}\\Color 
  	[.\textit{Red}\\\textbf{?} ]
	[.\textit{Blue}\\\textbf{Yes} ]
  ]
  [.\textit{Medium}\\Color 
  	[.\textit{Red}\\Noise
	  [.\textit{Loud}\\\textbf{No} ]
	  [.\textit{Quiet}\\\textbf{Yes} ]
	]
	[.\textit{Blue}\\\textbf{Yes} ]
  ]
  [.\textit{Large}\\\textbf{No} ]
]

\item If the event of missing a datum is uniformly random over all the
attributes, then it doesn't harm if we simply delete that line. 

\end{enumerate}

\section*{Problem 5}

\begin{enumerate}[(a)]

\item $\min \dfrac{1}{3} \pi r^2 h$ such that $A = \pi rs + \pi r^2$,
$s^2 = r^2 + h^2$.

Use Lagrange multiplier, $H = \frac{1}{3} \pi r^2 h + \lambda(\pi rs +
\pi r^2 - A) + \gamma (r^2 + h^2 - s^2)$.

$\frac{\partial H}{\partial r} = \frac{2}{3} \pi r h + \lambda(\pi s +
2 \pi r) + 2 \gamma r = 0$

$\frac{\partial H}{\partial h} = \frac{1}{3} \pi r^2 + 2 \gamma h = 0$

\end{enumerate}

\section*{Problem 6}

Each node has $m$ possible attributes. So there are no more than
$m^n$ configuration of the decision tree. The number of different
classifiers is also no more than $m^n$.

We know that for data set with size of $k$, a decision tree should
have $2^k$ decisions. So the VC dimension is $\log_2{m^n} = n
\log_2{m} = O(n\log(m))$.

\section*{Problem 7}

The smallest positive integer p is 2.

When $p = 1$, $k(x, x_i) = 1 + x^T x_i = 1 + x_1 x_{i1} + x_2 x_{i2}$.
So $\Phi(x) = (1, x_1, x_2)^T$. Clearly, as $(x_1, x_2)$ cannot be
separated by SVM, adding a bias of 1 doesn't help either.

When $p = 2$ --- same case as the class note --- $k(x, x_i) = (1 + x^T
x_i)^2 = (1 + x_1 x_{i1} + x_2 x_{i2})^2 = 1 + x_1^2 x_{i1}^2 + x_2^2
x_{i2}^2 + 2x_1x_{i1} + 2x_2x_{i2} + 2x_1x_{i1}x_2x_{i2}$. Then
$\Phi(x) = (1, x_1^2, x_2^2, \sqrt{2}x_1, \sqrt{2}x_2,
\sqrt{2}x_1x_2)^T$.

% $\Phi((1, 1)^T) = (1, 1, 1, \sqrt{2}, \sqrt{2}, \sqrt{2})^T$.
% 
% $\Phi((1, -1)^T) = (1, 1, 1, \sqrt{2}, -\sqrt{2}, -\sqrt{2})^T$.
% 
% $\Phi((-1, 1)^T) = (1, 1, 1, -\sqrt{2}, \sqrt{2}, -\sqrt{2})^T$.
% 
% $\Phi((-1, -1)^T) = (1, 1, 1, -\sqrt{2}, -\sqrt{2}, \sqrt{2})^T$.

Let $\lambda = \begin{pmatrix}
\lambda_1\\\lambda_2\\\lambda_3\\\lambda_4 \end{pmatrix}$.
$Q(\lambda) = I\lambda - \frac{1}{2} \lambda^T A \lambda $, where $A =
\begin{pmatrix}
9 & -1 & -1 & 1\\
-1 & 9 & 1 & -1\\
-1 & 1 & 9 & -1\\
1 & -1 & -1 & 9
\end{pmatrix}
$, the elements of which are computed by the kernel function. The
solution is $\lambda_i = \frac{1}{8}$ for $i = 1,2,3,4$.  $Q(\lambda)
= \frac{1}{4}$. 

\hfill

Using a value of $p$ larger than minimum would unnecessarily map the
data to higher dimension. The classifier becomes more nonlinear in
lower dimension, and thus may overfit the data.

\end{document}
